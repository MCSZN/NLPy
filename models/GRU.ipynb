{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import collections\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils & Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Dataframe to DataSet to DataLoader\n",
    "data = pd.read_json(\"inputs/cooking/train.json\")\n",
    "\n",
    "# String to Int\n",
    "vocab = list({word for sentence in data.ingredients for word in sentence})\n",
    "stoi = collections.defaultdict(lambda: len(vocab),{string:integer for integer,string in enumerate(vocab)})\n",
    "padIndex = len(vocab)+1\n",
    "\n",
    "# Y => Categories\n",
    "data.cuisine = data.cuisine.astype(\"category\")\n",
    "Y_unique = len(data.cuisine.cat.categories)\n",
    "\n",
    "# convert each ingredient in each recipe to its coresponding int\n",
    "data[\"X\"] = data.ingredients.apply(lambda l: np.array([stoi[s] for s in l]))\n",
    "\n",
    "#Dictionary which converts cuisine index to string value\n",
    "itos = {i:c for i,c in enumerate(data.cuisine.cat.categories)}\n",
    "    \n",
    "#Split train/valid sets\n",
    "np.random.seed(1)\n",
    "train, val = train_test_split(data, test_size=0.15)\n",
    "assert len(train) + len(val) == len(data)\n",
    "\n",
    "\n",
    "class MakeDS(torch.utils.data.Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X, self.y = X,y\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,index): \n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# Create three different datasets. fullDS contains all rows in training data\n",
    "dataset = MakeDS(data.X.values, data.cuisine.cat.codes.values)\n",
    "trainset = MakeDS(train.X.values, train.cuisine.cat.codes.values)\n",
    "valset = MakeDS(val.X.values, val.cuisine.cat.codes.values)\n",
    "\n",
    "# Custom collate function which takes a batch of samples and embeds them in a tensor (sequence length,batch size) \n",
    "# padded out to the max ingredient list length of the batch\n",
    "def collate(samples):\n",
    "    batchsize = len(samples)\n",
    "    maxLen = max(len(s[0]) for s in samples)\n",
    "    out = torch.zeros(maxLen,batchsize,dtype=torch.long) + padIndex\n",
    "    for i,s in enumerate(samples):\n",
    "        out[:len(s[0]),i] = torch.tensor(s[0],dtype=torch.long)\n",
    "    return out.to(device), torch.tensor([s[1] for s in samples],dtype=torch.long).to(device)\n",
    "\n",
    "#Create the dataloaders\n",
    "batchsize = 32\n",
    "trainDL = torch.utils.data.DataLoader(trainset,batchsize,collate_fn=collate,shuffle=False)\n",
    "valDL = torch.utils.data.DataLoader(valset,batchsize,collate_fn=collate)\n",
    "fullDL = torch.utils.data.DataLoader(dataset,batchsize,shuffle=True,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He init for embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/fastai/fastai/blob/master/fastai/layers.py#L116, \n",
    "# implements initialization for the embedding layer\n",
    "def trunc_normal_(x:torch.tensor, mean:float=0., std:float=1.) -> torch.tensor:\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,vocabSize,embSize,hiddenSize,nlayers,Y_unique):\n",
    "        super().__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nlayers = nlayers\n",
    "        # create ndimensional matrix for word embedding using pytorch\n",
    "        self.ingredEmb = torch.nn.Embedding(vocabSize,embSize)\n",
    "        # implement dropout\n",
    "        self.embDropout = torch.nn.Dropout(0.5)\n",
    "        with torch.no_grad(): trunc_normal_(self.ingredEmb.weight, std=0.01) \n",
    "        # Use He initilization on the embedding layer\n",
    "        # GRU(input_size, hidden_size, num_layers, (dropout(float), bias(bool), bidirectional(bool)))\n",
    "        self.ingredEnc = torch.nn.GRU(embSize,hiddenSize,nlayers,dropout=0.5)\n",
    "        self.encDropout = torch.nn.Dropout(0.5)\n",
    "        # linear out\n",
    "        self.out = torch.nn.Linear(hiddenSize*2,Y_unique)\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        # get dims\n",
    "        sl, batchsize = inp.size()\n",
    "        # upadte input put it in embedding\n",
    "        inp = self.embDropout(self.ingredEmb(inp))\n",
    "        # get encodding and hidden\n",
    "        enc,h = self.ingredEnc(inp,torch.zeros(self.nlayers,batchsize,self.hiddenSize).to(device))\n",
    "    \n",
    "        # using a bidrectional GRU,\n",
    "        # concat the forward state to the backward state, then pass it to the output layer\n",
    "        return self.out(self.encDropout(torch.cat([h[-2],h[-1]],dim=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net(number individual tokens, \n",
    "#            embedding size, \n",
    "#            hidden size,\n",
    "#            number layers, numbers possible outputs = number cuisines)\n",
    "model = RNN(vocabSize= len(vocab)+2,\n",
    "                   embSize = 10,\n",
    "                   hiddenSize = 9,\n",
    "                   nlayers = 10,\n",
    "                   Y_unique = Y_unique).to(device)\n",
    "\n",
    "\n",
    "#Grab a batch from the dataloader, and pass it through the model to make sure the output shape is correct\n",
    "x,y = next(iter(trainDL))\n",
    "\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and printing func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the average accuracy of a batch\n",
    "\n",
    "def batchAccuracy(preds,target):\n",
    "    preds = torch.softmax(preds,dim=1)\n",
    "    preds = torch.argmax(preds,dim=1)\n",
    "    o = (preds == target).sum().item()\n",
    "    return o / len(preds)\n",
    "\n",
    "# training function\n",
    "\n",
    "def training(model,epochs,lr,trainDL,valDL=None):\n",
    "    lossFn = torch.nn.functional.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,amsgrad=True,weight_decay=5e-4)\n",
    "\n",
    "    for e in tqdm_notebook(range(epochs)):\n",
    "        model.train()\n",
    "        with tqdm_notebook(iter(trainDL),leave=False) as t:\n",
    "            bloss, n = 0.0,0\n",
    "            for x,y in t:\n",
    "                pred = model(x)\n",
    "                loss = lossFn(pred,y)\n",
    "                bloss += loss.item()\n",
    "                n += 1\n",
    "                t.set_postfix({\"loss\": bloss / n})\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {e+1} Training Set Loss: {bloss / n}\")\n",
    "        if valDL is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                loss,accuracy,n =0.0,0.0,0\n",
    "                for x,y in tqdm_notebook(iter(valDL),leave=False):\n",
    "                    pred = model(x)\n",
    "                    loss += lossFn(pred,y)\n",
    "                    accuracy += batchAccuracy(pred,y)\n",
    "                    n += 1\n",
    "                print(f\"Validation Set Loss: {loss / n}, Accuracy: {accuracy / n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train (model, epochs, learning rate, dataloader)\n",
    "training(model,12,1e-2,trainDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
